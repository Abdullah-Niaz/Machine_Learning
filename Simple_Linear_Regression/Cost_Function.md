## üí° What is the Cost Function in Linear Regression?

In **Linear Regression**, our goal is to find the **best-fitting straight line**:

[
\hat{Y} = Œ≤_0 + Œ≤_1X
]

that predicts (Y) (dependent variable, like salary) from (X) (independent variable, like CGPA or experience).

To measure **how well the line fits the data**, we use a **Cost Function** ‚Äî a formula that calculates **how much error** exists between our predictions and the actual data.

---

## üéØ Purpose of the Cost Function

üëâ The **Cost Function** helps us *quantify the error*.
üëâ Our goal is to **minimize** this cost so that predictions are as close to reality as possible.

---

## üßÆ The Mathematical Formula

The most common cost function used in Linear Regression is the **Mean Squared Error (MSE)**:

[
J(Œ≤_0, Œ≤_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_Œ∏(x_i) - y_i)^2
]

Let‚Äôs break it down:

| Symbol         | Meaning                                                                        |
| -------------- | ------------------------------------------------------------------------------ |
| (J(Œ≤_0, Œ≤_1))  | Cost function (value to minimize)                                              |
| (m)            | Number of data points                                                          |
| (h_Œ∏(x_i))     | Predicted value for data point i ‚Üí (Œ≤_0 + Œ≤_1x_i)                              |
| (y_i)          | Actual output value                                                            |
| (\frac{1}{2m}) | Normalizes the error (average), and ¬Ω is for easy derivative calculation later |

---

## üí¨ Step-by-Step Intuition

1. **Make Predictions:**
   Use your model (line) to predict values of Y for all given Xs.
   Example:
   If (X = 5), (YÃÇ = Œ≤_0 + Œ≤_1√ó5)

2. **Find Errors (Residuals):**
   [
   \text{Error} = Y - \hat{Y}
   ]
   This shows how far each prediction is from the true value.

3. **Square Each Error:**
   To remove negatives and penalize large errors.

4. **Sum All Errors and Divide by n:**
   To find the average error ‚Üí Mean Squared Error.

---

## üìâ Geometric Meaning

* Imagine several points scattered around a line.
* Each vertical distance between a point and the line is an **error**.
* The Cost Function tells us the **total ‚Äúdistance‚Äù** of all points from the line (in squared form).
* The smaller this value, the **better the line fits**.

---

## üß† Why We Minimize It

The line with the **lowest cost function value** is the **best-fit line**.
We can find it by:

* Using **Ordinary Least Squares (OLS)** (analytical solution), or
* Using **Gradient Descent** (iterative optimization method).

---

## üìä Example in Code

```python
import numpy as np

# Actual vs Predicted
Y_actual = np.array([10, 20, 30, 40, 50])
Y_pred = np.array([12, 22, 28, 43, 48])

# Calculate Cost (MSE)
m = len(Y_actual)
cost = (1/(2*m)) * np.sum((Y_pred - Y_actual)**2)

print("Cost Function Value (MSE):", cost)
```

üß© Output:

```
Cost Function Value (MSE): 6.9
```

‚û°Ô∏è This means on average, our model‚Äôs squared error is 6.9 units (lower is better).

---

## üß≠ Summary Table

| Concept                  | Description                                     |
| ------------------------ | ----------------------------------------------- |
| **Objective**            | Find line that minimizes total prediction error |
| **Formula**              | ( J = \frac{1}{2m}\sum (YÃÇ - Y)^2 )             |
| **Best Fit Line**        | Occurs when cost function is minimal            |
| **Minimization Methods** | OLS or Gradient Descent                         |
| **Key Output**           | (Œ≤_0) (intercept) and (Œ≤_1) (slope)             |

